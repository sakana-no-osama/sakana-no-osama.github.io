# -*- coding: utf-8 -*-
"""
KNG SAFE ãƒ•ãƒ«å¯¾å¿œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ v1
- ä¸è¦HTMLã®å®‰å…¨é€€é¿ï¼ˆå‰Šé™¤ã¯ã—ãªã„ï¼‰
- ãƒãƒ¼ãƒ åˆ¥HTMLã‚’èµ°æŸ»ã—ã¦é‡è¤‡åã‚’æ­£è¦åŒ–ã€æœ€å¤§å¾—ç‚¹ã§é›†è¨ˆ
- index_kngsafe_final.html ã‚’æ–°è¦ç”Ÿæˆï¼ˆæ—¢å­˜ index.html ã¯è§¦ã‚‰ãªã„ï¼‰
- ãƒ­ã‚°ã‚’è¡¨ç¤ºï¼ˆæ¤œå‡ºæ•° / ä»£è¡¨çš„ãªé‡è¤‡ / é€€é¿ä»¶æ•°ï¼‰

å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:
  BASE = /sdcard/Download/sakana-no-osama.github.io
  ã¤ã„ã§ã« Download ç›´ä¸‹ã® *.html ã‚‚å›åï¼ˆSWEEP_DOWNLOAD_ROOT=Trueï¼‰

ä¸Šæ›¸ãç¦æ­¢ / ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¿…é ˆ / 1ã‚³ãƒãƒ³ãƒ‰å®Œçµ
"""

import os, re, shutil, json
from datetime import datetime
from collections import defaultdict, Counter

# ====== è¨­å®š ======
BASE = "/sdcard/Download/sakana-no-osama.github.io"
DOWNLOAD_ROOT = "/sdcard/Download"

# Download ç›´ä¸‹ã® *.html ã‚‚ã€Œä¸è¦ç‰©ã€ã¨ã—ã¦é€€é¿ã™ã‚‹ã‹
SWEEP_DOWNLOAD_ROOT = True

# é€€é¿ï¼ˆå‰Šé™¤ã¯ã—ãªã„ï¼‰
TS = datetime.now().strftime("%Y%m%d_%H%M%S")
BACKUP_ROOT = f"/sdcard/Download/backup_kng_{TS}"
UNNEC_BASE = os.path.join(BACKUP_ROOT, "unnecessary_html_in_base")
UNNEC_ROOT = os.path.join(BACKUP_ROOT, "unnecessary_html_in_download")

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¸¸ã«æ–°è¦ä½œæˆï¼‰
OUTPUT_INDEX = os.path.join(BASE, "index_kngsafe_final.html")
OUTPUT_JSON  = os.path.join(BASE, f"kng_result_{TS}.json")

# æ®‹ã™ï¼ˆï¼é€€é¿ã—ãªã„ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
KEEP_PATTERNS = [
    r"^index.*\.html$",              # index ç³»
    r"^U15RANK.*\.html$",            # RANK ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã©
    r"^team_.*\.html$",              # ãƒãƒ¼ãƒ åˆ¥
]

# ====== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ======
def ensure_dir(d):
    os.makedirs(d, exist_ok=True)

def match_any(name, patterns):
    return any(re.search(p, name, flags=re.I) for p in patterns)

def list_html(dirpath):
    try:
        return [f for f in os.listdir(dirpath) if f.lower().endswith(".html")]
    except Exception:
        return []

def strip_tags(x):
    return re.sub(r"<[^>]*>", "", x, flags=re.S).strip()

def normalize_name(name):
    n = name
    # å…¨è§’â†’åŠè§’ï¼ˆç°¡ä¾¿ç‰ˆï¼‰
    table = str.maketrans({
        "ã€€":" ", "â€“":"-", "ï¼":"-", "â€•":"-", "â€":"-",
    })
    n = n.translate(table)
    # ã‚ˆãã‚ã‚‹è¡¨è¨˜ã‚†ã‚Œ
    n = n.replace("ã€€", " ").replace(" ", " ")  # NBSP
    n = re.sub(r"\s+", " ", n).strip()
    n = n.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
    # ã‚ªã‚¦ãƒ³ã‚´ãƒ¼ãƒ«çµ±ä¸€
    if re.fullmatch(r"(ã‚ªã‚¦ãƒ³ã‚´ãƒ¼ãƒ«|ï¼¯ï¼§|OG|og)", n, flags=re.I):
        return "OG"
    # è¨˜å·ãƒ»ç©ºç™½é™¤å»ï¼ˆäººåã®é‡è¤‡å¯¾ç­–ï¼‰
    n2 = re.sub(r"[^\w\u3040-\u30FF\u4E00-\u9FFF]+", "", n)
    return n2

def parse_team_rows(html):
    """
    æœŸå¾…ãƒ†ãƒ¼ãƒ–ãƒ«: 4åˆ—ï¼ˆé †ä½/é¸æ‰‹å/ãƒãƒ¼ãƒ /å¾—ç‚¹ï¼‰ or 3åˆ—ï¼ˆé †ä½ãªã—ï¼‰
    """
    rows = []
    # tr ã‚’æŠœã
    for tr in re.findall(r"<tr[^>]*>(.*?)</tr>", html, flags=re.S|re.I):
        tds = re.findall(r"<td[^>]*>(.*?)</td>", tr, flags=re.S|re.I)
        if not tds:
            continue
        cols = [strip_tags(x) for x in tds]
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚¹ã‚­ãƒƒãƒ—
        head = "".join(cols)
        if "é¸æ‰‹" in head and "å¾—ç‚¹" in head:
            continue
        # åˆ—å½“ã¦
        name, team, goals = None, None, None
        if len(cols) >= 4:
            # [é †ä½, é¸æ‰‹, ãƒãƒ¼ãƒ , å¾—ç‚¹]
            name, team, goals = cols[1], cols[2], cols[3]
        elif len(cols) == 3:
            # [é¸æ‰‹, ãƒãƒ¼ãƒ , å¾—ç‚¹] ã®æƒ³å®š
            name, team, goals = cols[0], cols[1], cols[2]
        else:
            continue
        # å¾—ç‚¹ã¯æ•°å­—ã®ã¿æŠ½å‡º
        m = re.search(r"\d+", goals)
        if not m:
            continue
        g = int(m.group())
        rows.append((name.strip(), team.strip(), g))
    return rows

def extract_table(path):
    try:
        html = open(path, encoding="utf-8").read()
    except Exception:
        return []
    return parse_team_rows(html)

def is_team_file(fname):
    return fname.lower().startswith("team_") and fname.lower().endswith(".html")

# ====== 1) ä¸è¦HTMLã®å®‰å…¨é€€é¿ ======
def sweep_unnecessary():
    moved = 0
    ensure_dir(UNNEC_BASE)
    files = list_html(BASE)
    for f in files:
        if match_any(f, KEEP_PATTERNS):
            continue
        src = os.path.join(BASE, f)
        dst = os.path.join(UNNEC_BASE, f)
        try:
            shutil.move(src, dst)
            moved += 1
        except Exception:
            pass

    moved_root = 0
    if SWEEP_DOWNLOAD_ROOT:
        ensure_dir(UNNEC_ROOT)
        for f in list_html(DOWNLOAD_ROOT):
            # BASE é…ä¸‹ã® index / team ã«é–¢ä¿‚ãªã„ã€Œç›´ä¸‹ *.htmlã€ã¯é€€é¿
            # ï¼ˆèª¤çˆ†é˜²æ­¢ã®ãŸã‚ã€sakana-no-osama.github.io ã®å¤–ã«ã‚ã‚‹ *.html ã‚’æ‹¾ã†ï¼‰
            src = os.path.join(DOWNLOAD_ROOT, f)
            if os.path.isdir(src):
                continue
            # BASE ç›´ä¸‹ã«åŒåãŒã‚ã‚‹ãªã‚‰è§¦ã‚‰ãªã„
            if os.path.exists(os.path.join(BASE, f)):
                continue
            try:
                shutil.move(src, os.path.join(UNNEC_ROOT, f))
                moved_root += 1
            except Exception:
                pass

    return moved, moved_root

# ====== 2) é›†è¨ˆï¼ˆé‡è¤‡åã¯æ­£è¦åŒ–ã—æœ€å¤§å¾—ç‚¹æ¡ç”¨ï¼‰ ======
def aggregate():
    per_file_counts = {}
    totals = defaultdict(int)
    shown_name = {}   # æ­£è¦åŒ–å -> è¡¨ç¤ºå
    name_team  = {}   # æ­£è¦åŒ–å -> æ¡ç”¨ãƒãƒ¼ãƒ 
    conflicts  = []   # ãƒãƒ¼ãƒ ãŒç•°ãªã‚‹é‡è¤‡ã®è¨˜éŒ²

    team_files = [f for f in list_html(BASE) if is_team_file(f)]
    scanned = 0
    for f in team_files:
        scanned += 1
        rows = extract_table(os.path.join(BASE, f))
        per_file_counts[f] = len(rows)
        for name, team, g in rows:
            key = normalize_name(name)
            # è¡¨ç¤ºåã¯ã‚ˆã‚Šé•·ã„æ–¹ã‚’æ¡ç”¨ï¼ˆæ¼¢å­—å„ªå…ˆæƒ³å®šï¼‰
            if key not in shown_name or len(name) > len(shown_name[key]):
                shown_name[key] = name
            # ãƒãƒ¼ãƒ ãŒé•ã†å ´åˆã®è¨˜éŒ²ï¼ˆæ¡ç‚¹ã¯æœ€å¤§å€¤ï¼‰
            if key in name_team and name_team[key] != team:
                conflicts.append((shown_name[key], name_team[key], team))
            name_team[key] = team if (key not in name_team or g >= totals[key]) else name_team[key]
            if g > totals[key]:
                totals[key] = g

    return {
        "scanned": scanned,
        "team_files": team_files,
        "per_file_counts": per_file_counts,
        "totals": dict(totals),
        "shown_name": shown_name,
        "name_team": name_team,
        "conflicts": conflicts,
    }

# ====== 3) indexï¼ˆå®Œæˆç‰ˆï¼‰ç”Ÿæˆ ======
def build_index(result):
    totals = result["totals"]
    shown  = result["shown_name"]
    name_team = result["name_team"]
    per_file = result["per_file_counts"]

    ranking = sorted(totals.items(), key=lambda x: (-x[1], x[0]))
    files = result["team_files"]
    # ãƒãƒ¼ãƒ ãƒªãƒ³ã‚¯ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åæ˜‡é †ï¼‰
    links = "\n".join(
        f'<li><a href="{f}">{strip_tags(f.replace("team_","").replace(".html",""))}</a></li>'
        for f in sorted(files)
    )

    # è¨ºæ–­ãƒ­ã‚°
    per_log = "\n".join(
        f"<li>{f}: {c}å</li>" for f, c in sorted(per_file.items())
    )

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨
    rows = []
    for i, (key, g) in enumerate(ranking, 1):
        name = shown.get(key, key)
        team = name_team.get(key, "")
        rows.append(f"<tr><td>{i}</td><td>{name}</td><td>{team}</td><td>{g}</td></tr>")
    table = "\n".join(rows)

    html = f"""<!doctype html>
<html><head><meta charset="utf-8">
<title>U-15 å¾—ç‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆKNG æœ€çµ‚ç‰ˆï¼‰</title>
<style>
body{{font-family:sans-serif}}
table{{border-collapse:collapse}}
th,td{{border:1px solid #ccc;padding:6px 10px}}
th{{background:#f6f6f6}}
small{{color:#666}}
</style></head><body>
<h2>U-15 å¾—ç‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆè‡ªå‹•é›†è¨ˆãƒ»KNGï¼‰</h2>
<p>æœ€çµ‚æ›´æ–°: {datetime.now().strftime("%Y/%m/%d %H:%M:%S")} / ã‚¹ã‚­ãƒ£ãƒ³: {result["scanned"]}ãƒ•ã‚¡ã‚¤ãƒ«</p>

<h3>å¾—ç‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°</h3>
<table>
<tr><th>é †ä½</th><th>é¸æ‰‹å</th><th>ãƒãƒ¼ãƒ </th><th>å¾—ç‚¹</th></tr>
{table}
</table>

<h3>ãƒãƒ¼ãƒ åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆãƒªãƒ³ã‚¯ï¼‰</h3>
<ul>
{links}
</ul>

<h3>è¨ºæ–­ãƒ­ã‚°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åˆ¥æ¤œå‡ºäººæ•°ï¼‰</h3>
<ul>
{per_log}
</ul>

<p><small>â€» é‡è¤‡åã¯æ­£è¦åŒ–ã—ã€Œæœ€å¤§å¾—ç‚¹ã€ã‚’æ¡ç”¨ã€‚ç•°ãƒãƒ¼ãƒ é‡è¤‡ã¯ name_team ã®æœ€æ–°ãƒãƒ¼ãƒ ã§è¡¨ç¤ºã€‚</small></p>
</body></html>"""
    open(OUTPUT_INDEX, "w", encoding="utf-8").write(html)
    return OUTPUT_INDEX

# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    if not os.path.isdir(BASE):
        print(f"âŒ BASE ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {BASE}")
        return

    ensure_dir(BACKUP_ROOT)
    m1, m2 = sweep_unnecessary()
    print(f"ğŸ“¦ é€€é¿: BASEå†… {m1} ä»¶ / Downloadç›´ä¸‹ {m2} ä»¶ â†’ {BACKUP_ROOT}")

    result = aggregate()

    # ã–ã£ãã‚Šãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    totals = result["totals"]
    if not totals:
        print("âš ï¸ å¾—ç‚¹ãƒ‡ãƒ¼ã‚¿ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã€‚team_*.html ã®è¡¨æ§‹é€ ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        return
    print("ğŸ‘€ ä¸Šä½ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:", Counter(totals).most_common(5))

    out = build_index(result)
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"âœ… å‡ºåŠ›: {out}ï¼ˆ{len(totals)}åï¼‰")
    print(f"ğŸ“ ãƒ­ã‚°: {OUTPUT_JSON}")
    print("ğŸ‘‰ ãƒ–ãƒ©ã‚¦ã‚¶ã§ç›´æ¥é–‹ã: file://" + out)

if __name__ == "__main__":
    main()
